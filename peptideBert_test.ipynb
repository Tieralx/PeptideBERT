{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tmp_cases\\DEV\\git\\PeptideBERT\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from data.dataloader import load_data\n",
    "from model.network import create_model, cri_opt_sch\n",
    "from model.utils import train, validate, test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from convert_encodings import m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Preparation\n",
    "\n",
    "create a `csv` file with the following format:\n",
    "```csv\n",
    "sequence,label\n",
    "AAAAAAA,1\n",
    "LLLLLLL,0\n",
    "CCCCCCC,0\n",
    "DDDDDDD,1\n",
    "```\n",
    "where `sequence` is the peptide sequence and `label` is the binary label (0 or 1). Save this file as `custom_data.csv` inside the `data` directory. Now, run the following cell (edit `task_name` as desired) to convert the `csv` file to the format required by PeptideBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'peptidebert_MNA_equipo1'\n",
    "\n",
    "# read data\n",
    "seqs, labels = [], []\n",
    "with open('./data/custom_data.csv', 'r', encoding = 'UTF-8') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        seq, label = line.strip().split(',')\n",
    "        seqs.append(seq)\n",
    "        labels.append(int(label))\n",
    "\n",
    "MAX_LEN = max(map(len, seqs))\n",
    "# convert to tokens\n",
    "mapping = dict(zip(\n",
    "    ['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]','L',\n",
    "    'A','G','V','E','S','I','K','R','D','T','P','N',\n",
    "    'Q','F','Y','M','H','C','W','X','O','B','U','J','Z'],\n",
    "    range(97)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data, neg_data = [], []\n",
    "for i in range(len(seqs)):\n",
    "    seq = [mapping[c] for c in seqs[i]] \n",
    "    seq.extend([0] * (MAX_LEN - len(seq)))  # padding to max length\n",
    "    if labels[i] == 1:\n",
    "        pos_data.append(seq)\n",
    "    else:\n",
    "        neg_data.append(seq)\n",
    "\n",
    "pos_data = np.array(pos_data)\n",
    "neg_data = np.array(neg_data)\n",
    "\n",
    "np.savez(\n",
    "    f'./data/{task_name}-positive.npz',\n",
    "    arr_0=pos_data\n",
    ")\n",
    "np.savez(\n",
    "    f'./data/{task_name}-negative.npz',\n",
    "    arr_0=neg_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train-Val-Test Split\n",
    "Now, we want to combine the positive and negative samples, shuffle them and split them into 3 non-overlapping sets - train, validation, and test.\n",
    "\n",
    "To do so, edit the `main` function inside `./data/split_augment.py` file (comment existing calls to `split_data` and add the line `split_data('REPLACE_WITH_TASK_NAME')`) and run the following cell, this will create sub-directories (inside the `data` directory) for the custom dataset and place the subsets (train, validation, test) inside it.\n",
    "\n",
    "Additionally, if you want to augment the dataset, you can do so by editing `./data/split_augment.py` file. You can call the `augment_data` function from the `main` function like so: `augment_data('REPLACE_WITH_TASK_NAME')`.\n",
    "\n",
    "Further, to change/experiment with the augmentation techniques applied, you can edit the `augment_data` function. Comment/uncomment the call to any of the augmentation functions (such as `random_replace`, `random_delete`, etc.) as desired, change the factor for augmentation as desired. Do keep in mind that for each augmentation applied, you have to call the `combine` function. For example, if you want to apply the `random_swap` augmentation with a `factor` of 0.2, you can add `new_inputs, new_labels = random_swap(inputs, labels, 0.2)` followed by `inputs, labels = combine(inputs, labels, new_inputs, new_labels)` to merge the augmented dataset into the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(task):\n",
    "    with np.load(f'./data/{task}-positive.npz') as pos,\\\n",
    "         np.load(f'./data/{task}-negative.npz') as neg:\n",
    "        pos_data = pos['arr_0']\n",
    "        neg_data = neg['arr_0']\n",
    "\n",
    "    input_ids = np.vstack((\n",
    "        pos_data,\n",
    "        neg_data\n",
    "    ))\n",
    "\n",
    "    labels = np.hstack((\n",
    "        np.ones(len(pos_data)),\n",
    "        np.zeros(len(neg_data))\n",
    "    ))\n",
    "\n",
    "    train_val_inputs, test_inputs, train_val_labels, test_labels = train_test_split(\n",
    "        input_ids, labels, test_size=0.1\n",
    "    )\n",
    "\n",
    "    train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "        train_val_inputs, train_val_labels, test_size=0.1\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(f'./data/{task}'):\n",
    "        os.mkdir(f'./data/{task}')\n",
    "\n",
    "    np.savez(\n",
    "        f'./data/{task}/train.npz',\n",
    "        inputs=train_inputs,\n",
    "        labels=train_labels\n",
    "    )\n",
    "\n",
    "    np.savez(\n",
    "        f'./data/{task}/val.npz',\n",
    "        inputs=val_inputs,\n",
    "        labels=val_labels\n",
    "    )\n",
    "\n",
    "    np.savez(\n",
    "        f'./data/{task}/test.npz',\n",
    "        inputs=test_inputs,\n",
    "        labels=test_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(inputs, labels, new_inputs, new_labels):\n",
    "    new_inputs = np.vstack(new_inputs)\n",
    "    new_labels = np.hstack(new_labels)\n",
    "\n",
    "    inputs = np.vstack((inputs, new_inputs))\n",
    "    labels = np.hstack((labels, new_labels))\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def random_replace(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        num_to_replace = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_replace, replace=False)\n",
    "        ip[indices] = np.random.choice(np.arange(5, 25), num_to_replace, replace=True)\n",
    "\n",
    "        new_inputs.append(ip)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_delete(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        ip = list(ip[:unpadded_len])\n",
    "        num_to_delete = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_delete, replace=False)\n",
    "        for i in reversed(sorted(indices)):\n",
    "            ip.pop(i)\n",
    "        ip.extend([0] * (200 - len(ip)))\n",
    "\n",
    "        new_inputs.append(np.asarray(ip))\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_replace_with_A(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        num_to_replace = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_replace, replace=False)\n",
    "        ip[indices] = m2['A']\n",
    "\n",
    "        new_inputs.append(ip)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_swap(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        ip = list(ip[:unpadded_len])\n",
    "        num_to_swap = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(range(1, unpadded_len, 2), num_to_swap, replace=False)\n",
    "        for i in indices:\n",
    "            ip[i-1], ip[i] = ip[i], ip[i-1]\n",
    "        ip.extend([0] * (200 - len(ip)))\n",
    "\n",
    "        new_inputs.append(np.asarray(ip))\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "\n",
    "def random_insertion_with_A(inputs, labels, factor):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        ip = inputs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        try:\n",
    "            unpadded_len = np.where(ip == 0)[0][0]\n",
    "        except IndexError:\n",
    "            unpadded_len = len(ip)\n",
    "        ip = list(ip[:unpadded_len])\n",
    "        num_to_insert = round(unpadded_len * factor)\n",
    "        indices = np.random.choice(unpadded_len, num_to_insert, replace=False)\n",
    "        for i in indices:\n",
    "            ip.insert(i, m2['A'])\n",
    "        if len(ip) < 200:\n",
    "            ip.extend([0] * (200 - len(ip)))\n",
    "        elif len(ip) > 200:\n",
    "            ip = ip[:200]\n",
    "\n",
    "        new_inputs.append(np.asarray(ip))\n",
    "        new_labels.append(label)\n",
    "\n",
    "    return new_inputs, new_labels\n",
    "\n",
    "def random_masking(sequences, mask_prob=0.15, mask_token_id=0):\n",
    "    masked_sequences = np.copy(sequences)\n",
    "    mask = np.random.rand(*sequences.shape) < mask_prob\n",
    "    masked_sequences[mask] = mask_token_id\n",
    "    return masked_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(task):\n",
    "    with np.load(f'./data/{task}/train.npz') as train:\n",
    "        inputs = train['inputs']\n",
    "        labels = train['labels']\n",
    "\n",
    "    # new_inputs1, new_labels1 = random_replace(inputs, labels, 0.02)\n",
    "    # new_inputs2, new_labels2 = random_delete(inputs, labels, 0.02)\n",
    "    # new_inputs3, new_labels3 = random_replace_with_A(inputs, labels, 0.02)\n",
    "    new_inputs4, new_labels4 = random_swap(inputs, labels, 0.02)\n",
    "    # new_inputs5, new_labels5 = random_insertion_with_A(inputs, labels, 0.02)\n",
    "    #new_inputs6, new_labels6 = random_masking(inputs, mask_prob=0.15, mask_token_id=0)\n",
    "\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs1, new_labels1)\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs2, new_labels2)\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs3, new_labels3)\n",
    "    inputs, labels = combine(inputs, labels, new_inputs4, new_labels4)\n",
    "    # inputs, labels = combine(inputs, labels, new_inputs5, new_labels5)\n",
    "    #inputs, labels = combine(inputs, labels, new_inputs6, new_labels6)\n",
    "\n",
    "    np.savez(\n",
    "        f'./data/{task}/train.npz',\n",
    "        inputs=inputs,\n",
    "        labels=labels\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data('hemo')\n",
    "split_data('sol')\n",
    "split_data('nf')\n",
    "# augment_data('sol')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Config\n",
    "Edit the `config.yaml` file and set the `task` parameter to `REPLACE_WITH_TASK_NAME`.\n",
    "\n",
    "Additionally, If you want to tweak the model before training, you can do so by editing `./model/network.py` and `config.yaml` files. `./model/network.py` contains the actual architecture of the model as well as the optimizer and scheduler used to train the model. `config.yaml` contains all the hyperparameters used for training, as well as which dataset to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training\n",
    "Now we are ready to train our model. Run the following cell to start the training procedure. This will save a checkpoint of the best model (on validation set) inside the `checkpoints` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(f'{\"=\"*30}{\"TRAINING\":^20}{\"=\"*30}')\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss = train(model, train_data_loader, optimizer, criterion, scheduler, device)\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch {epoch+1}/{config[\"epochs\"]} - Train Loss: {train_loss}\\tLR: {curr_lr}')\n",
    "        val_loss, val_acc = validate(model, val_data_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}/{config[\"epochs\"]} - Validation Loss: {val_loss}\\tValidation Accuracy: {val_acc}\\n')\n",
    "        scheduler.step(val_acc)\n",
    "        #if not config['debug']:\n",
    "        #    wandb.log({\n",
    "        #        'train_loss': train_loss, \n",
    "        #        'val_loss': val_loss, \n",
    "        #        'val_accuracy': val_acc, \n",
    "        #        'lr': curr_lr\n",
    "        #    })\n",
    "\n",
    "        if val_acc >= best_acc and not config['debug']:\n",
    "            best_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'acc': val_acc, \n",
    "                'lr': curr_lr\n",
    "            }, f'{save_dir}/model.pt')\n",
    "            print('Model Saved\\n')\n",
    "    #wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================        DATA        ==============================\n",
      "Batch size:  32\n",
      "Train dataset samples:  14946\n",
      "Validation dataset samples:  1661\n",
      "Test dataset samples:  1846\n",
      "Train dataset batches:  468\n",
      "Validation dataset batches:  52\n",
      "Test dataset batches:  58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tmp_cases\\DEV\\git\\PeptideBERT\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\tmp_cases\\DEV\\git\\PeptideBERT\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "#device = 'cpu'\n",
    "config = yaml.load(open('./config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "config['device'] = device\n",
    "\n",
    "train_data_loader, val_data_loader, test_data_loader = load_data(config)\n",
    "config['sch']['steps'] = len(train_data_loader)\n",
    "\n",
    "model = create_model(config)\n",
    "criterion, optimizer, scheduler = cri_opt_sch(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['debug']:\n",
    "    run_name = f'{config[\"task\"]}-{datetime.now().strftime(\"%m%d_%H%M\")}'\n",
    "    #wandb.init(project='PeptideBERT', name=run_name)\n",
    "\n",
    "    save_dir = f'./checkpoints/{run_name}'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    shutil.copy('./config.yaml', f'{save_dir}/config.yaml')\n",
    "    shutil.copy('./model/network.py', f'{save_dir}/network.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================      TRAINING      ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:36:32<00:00, 12.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.6587386612708752\tLR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:53<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Validation Loss: 0.6335006665724975\tValidation Accuracy: 63.576158940397356\n",
      "\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:33:26<00:00, 11.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train Loss: 0.6404607272428325\tLR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:54<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Validation Loss: 0.6280845862168533\tValidation Accuracy: 64.35881998795907\n",
      "\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [1:32:27<00:00, 11.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train Loss: 0.6329817889719946\tLR: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:52<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Validation Loss: 0.6127537024708894\tValidation Accuracy: 65.32209512341961\n",
      "\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silvalej\\AppData\\Local\\Temp\\ipykernel_25816\\2230136781.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'{save_dir}/model.pt')['model_state_dict'], strict=False)\n",
      "100%|██████████| 58/58 [01:00<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 63.92199349945828%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model()\n",
    "if not config['debug']:\n",
    "    model.load_state_dict(torch.load(f'{save_dir}/model.pt')['model_state_dict'], strict=False)\n",
    "test_acc = test(model, test_data_loader, device)\n",
    "print(f'Test Accuracy: {test_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
